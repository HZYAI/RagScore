# -*- coding: utf-8 -*-
"""RAGScore_AAS_P3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GqZ4MWUG_5wh7_jYvlZvxaaeCZ_qA-Qw
"""

!pip install requests tqdm
!pip install openai

from google.colab import drive
drive.mount('/content/drive')

import time, json, sys, os
from typing import Any, Dict, Tuple, Optional, List
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from tqdm import tqdm
import pandas as pd

# ==============================
# 配置
# ==============================
LOGIN_URL = "http://47.99.205.203:5004/login"
QUERY_URL = "http://47.99.205.203:5004/api/query"

USERNAME = "demo"
PASSWORD = "demo123"

QA_FILE = "/content/drive/My Drive/AI/RAGScore/generated_qas.jsonl"

# ==============================
# 更稳的 Session + 重试
# ==============================
session = requests.Session()
retry = Retry(
    total=3,
    backoff_factor=0.8,
    status_forcelist=[429, 500, 502, 503, 504],
    allowed_methods=frozenset(['GET', 'POST'])
)
adapter = HTTPAdapter(max_retries=retry, pool_connections=20, pool_maxsize=20)
session.mount("http://", adapter)
session.mount("https://", adapter)
session.headers.update({"Accept": "application/json"})

# ==============================
# 登录
# ==============================
login_resp = session.post(LOGIN_URL, data={"username": USERNAME, "password": PASSWORD}, timeout=(5, 20))
if login_resp.status_code != 200:
    print("登录失败：", login_resp.status_code, login_resp.text[:200])
    sys.exit(1)
print("✅ 登录成功！")

# ==============================
# 从 generated_qas.jsonl 读取问答
# ==============================
if not os.path.exists(QA_FILE):
    print(f"❌ 未找到问答文件: {QA_FILE}")
    sys.exit(1)

qa_list: List[Tuple[str, str]] = []
with open(QA_FILE, "r", encoding="utf-8") as f:
    for line in f:
        try:
            item = json.loads(line)
            q = item.get("question", "").strip()
            a = item.get("answer", "").strip()
            if q and a:
                qa_list.append((q, a))
        except json.JSONDecodeError:
            continue

print(f"✅ 从 {QA_FILE} 读取 {len(qa_list)} 条问答样本。")

# ==============================
# 工具函数
# ==============================
def parse_answer(res_json: Dict[str, Any]) -> Optional[str]:
    """尽可能多地兜底字段名，只返回 answer"""
    candidates = [
        res_json.get("answer"),
        res_json.get("response"),
        res_json.get("result"),
        res_json.get("msg"),
    ]
    if not any(candidates):
        data = res_json.get("data") or {}
        if isinstance(data, dict):
            candidates += [
                data.get("answer"),
                data.get("response"),
                data.get("result"),
            ]
    if not any(candidates):
        choices = res_json.get("choices")
        if isinstance(choices, list) and choices:
            msg = choices[0].get("message") or {}
            candidates.append(msg.get("content"))
        elif "output_text" in res_json:
            candidates.append(res_json.get("output_text"))
    answer = next((c for c in candidates if isinstance(c, str) and c.strip()), None)
    return answer or ""

def is_streaming(resp: requests.Response) -> bool:
    ct = resp.headers.get("Content-Type", "").lower()
    te = resp.headers.get("Transfer-Encoding", "").lower()
    return ("text/event-stream" in ct) or ("chunked" in te)

def ask_one(question: str) -> Tuple[str, Dict[str, Any]]:
    """对单个问题发起请求"""
    try:
        resp = session.post(QUERY_URL, json={"query": question}, timeout=(5, 40), stream=True)
        resp.raise_for_status()
        body = None
        if is_streaming(resp):
            chunks = []
            for chunk in resp.iter_content(chunk_size=4096, decode_unicode=True):
                if chunk:
                    chunks.append(chunk)
            body = "".join(chunks)
        else:
            body = resp.text

        if not body or not body.strip():
            return "", {"error": "Empty body with 200", "headers": dict(resp.headers)}

        try:
            res_json = json.loads(body)
        except Exception as je:
            return "", {"error": f"JSON parse error: {je}", "body_head": body[:500]}

        answer = parse_answer(res_json)
        return (answer or ""), res_json

    except requests.exceptions.Timeout:
        return "", {"error": "Timeout"}
    except requests.exceptions.RequestException as re:
        return "", {"error": f"HTTP error: {re}"}
    except Exception as e:
        return "", {"error": f"Unexpected: {e}"}

# ==============================
# 批量执行
# ==============================
server_answers, raw_response_list = [], []

for question, std_answer in tqdm(qa_list, desc="Querying Server"):
    ans, raw = ask_one(question)
    server_answers.append(ans)
    raw_response_list.append(raw)
    time.sleep(0.05)  # 轻度限速

# ==============================
# 保存结果
# ==============================
df = pd.DataFrame({
    "问题": [q for q, _ in qa_list],
    "标准答案": [a for _, a in qa_list],
    "服务器答案": server_answers
})
df.to_csv("rag_test_result.csv", index=False, encoding="utf-8-sig")
print("✅ 全部测试完成，已保存 rag_test_result.csv")

with open("rag_raw_responses.jsonl", "w", encoding="utf-8") as f:
    for r in raw_response_list:
        f.write(json.dumps(r, ensure_ascii=False) + "\n")
print("✅ 原始响应已保存 rag_raw_responses.jsonl")

df

!pip -q install aiohttp tqdm pandas nest_asyncio

"""Concurrent API Call

"""

!pip install openai
from openai import OpenAI
import os
# ---- 设置OpenAI API密钥 ----
os.environ["OPENAI_API_KEY"] ="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"
api_key = os.environ.get("OPENAI_API_KEY")
client = OpenAI()

!pip install xlsxwriter

import os
import re
import pandas as pd
from tqdm import tqdm
from openai import OpenAI
from typing import Dict, Any, Optional, Tuple

# ===== 1) 读取已有结果 =====
# 优先使用内存中的 df；若不存在则读取上一步保存的 CSV
try:
    df  # noqa: F821
except NameError:
    try:
        df = pd.read_csv("rag_test_result.csv")
    except FileNotFoundError:
        print("Error: rag_test_result.csv not found. Please run the previous cell first.")
        sys.exit(1)


# 校验必须字段
required_cols = {"问题", "标准答案", "服务器答案"}
missing = required_cols - set(df.columns)
if missing:
    raise ValueError(f"缺少必要列: {missing}；请确认上一个 block 生成的 df/CSV。")

# ===== 2) OpenAI 初始化（不要硬编码key）=====
# 请在环境中设置：export OPENAI_API_KEY="your-key"
client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

# ===== 3) 定义评分函数（只比较两段文本，不发服务器请求）=====
def llm_rate_answer(server_answer: str, std_answer: str) -> int:
    """
    使用 GPT-4o-mini 评分：0-100。
    只返回整数分；解析失败返回 50。
    """
    if not isinstance(server_answer, str) or not server_answer.strip():
        return 0
    if not isinstance(std_answer, str) or not std_answer.strip():
        return 0

    prompt = (
        "You are an evaluator. Compare the two answers for semantic equivalence and factual correctness. "
        "Return ONLY an integer from 0 to 100 (no text). "
        "100 = fully correct and equivalent; 0 = completely wrong or irrelevant."
        f"\n\n[Standard Answer]\n{std_answer}\n\n[Server Answer]\n{server_answer}\n\nScore:"
    )

    try:
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "Return only an integer score between 0 and 100."},
                {"role": "user", "content": prompt},
            ],
            max_tokens=10,
            temperature=0.0,
        )
        content = (resp.choices[0].message.content or "").strip()

        # 解析第一个 0-100 的整数
        m = re.search(r"\b(100|[1-9]?\d)\b", content)
        score = int(m.group(1)) if m else 50
        return max(0, min(100, score))
    except Exception as e:
        print(f"评分调用失败：{e}")
        return 50

# ===== 4) 批量评分，不再调用你的 QUERY 接口 =====
# Check if '准确性评分' column already exists. If not, perform scoring.
if '准确性评分' not in df.columns:
    scores = []
    print("Calculating accuracy scores...")
    for row in tqdm(df.itertuples(index=False), total=len(df)):
        server_ans = getattr(row, "服务器答案")
        std_ans = getattr(row, "标准答案")
        score = llm_rate_answer(server_ans, std_ans)
        scores.append(score)
    df["准确性评分"] = scores
else:
    print("'准确性评分' column already exists. Skipping scoring.")


# ===== 5) 计算整体测试结果 =====
total_questions = len(df)
answered_questions = df['服务器答案'].astype(str).str.strip().astype(bool).sum() # Count non-empty server answers
overall_accuracy_percentage = df['准确性评分'].mean() if total_questions > 0 else 0

scored_over_80 = (df['准确性评分'] > 80).sum()
scored_60_to_80 = ((df['准确性评分'] >= 60) & (df['准确性评分'] <= 80)).sum()
scored_under_60 = (df['准确性评分'] < 60).sum()

# 筛选出得分低于 60 的问题
questions_under_60_df = df[df['准确性评分'] < 60][['问题', '标准答案', '服务器答案', '准确性评分']]

# ===== 6) 保存结果到CSV，包含总览tab =====
out_path = "rag_test_result_scored.xlsx"

# Create a summary DataFrame
summary_data = {
    'Metric': [
        'Total Questions',
        'Answered Questions',
        'Overall Accuracy (%)',
        'Scores > 80',
        'Scores 60-80',
        'Scores < 60'
    ],
    'Value': [
        total_questions,
        answered_questions,
        f"{overall_accuracy_percentage:.2f}",
        scored_over_80,
        scored_60_to_80,
        scored_under_60
    ]
}
summary_df = pd.DataFrame(summary_data)

# Use ExcelWriter to save multiple sheets
with pd.ExcelWriter(out_path, engine='xlsxwriter') as writer:
    # Save the detailed results
    df.to_excel(writer, sheet_name='Detailed Results', index=False)

    # Save the summary
    summary_df.to_excel(writer, sheet_name='Overall Summary', index=False)

    # Save questions under 60
    if not questions_under_60_df.empty:
        questions_under_60_df.to_excel(writer, sheet_name='Questions < 60', index=False)
    else:
        # Create an empty sheet if no questions are under 60
        pd.DataFrame({'Message': ['No questions scored under 60.']}).to_excel(writer, sheet_name='Questions < 60', index=False)


print(f"评分完成，已保存 {out_path} (包含 'Detailed Results', 'Overall Summary', and 'Questions < 60' sheets)")

# 如需预览
df.head(5)