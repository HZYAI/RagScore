{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ RAGScore Demo - Quick RAG Evaluation\n",
        "\n",
        "This notebook demonstrates how to use RAGScore to evaluate your RAG system.\n",
        "\n",
        "**Features:**\n",
        "- ‚úÖ Works in Google Colab and Jupyter\n",
        "- ‚úÖ Supports local LLMs (Ollama) and cloud APIs\n",
        "- ‚úÖ One-liner RAG testing with `quick_test()`\n",
        "- ‚úÖ Returns pandas DataFrames for easy analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup\n",
        "\n",
        "### Option A: Use with Cloud LLM (OpenAI, Anthropic, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install RAGScore\n",
        "!pip install -q ragscore[notebook,openai]\n",
        "\n",
        "# Set your API key\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"  # Replace with your key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option B: Use with Local LLM (Ollama) - FREE & Private!\n",
        "\n",
        "Run this cell to set up Ollama in Colab (takes ~2 minutes):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ü™Ñ Magic Cell: Install and start Ollama in Colab\n",
        "# This runs Ollama as a background process\n",
        "\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Start Ollama server in background\n",
        "process = subprocess.Popen(\n",
        "    \"ollama serve\",\n",
        "    shell=True,\n",
        "    stdout=subprocess.DEVNULL,\n",
        "    stderr=subprocess.DEVNULL\n",
        ")\n",
        "time.sleep(5)  # Wait for server to start\n",
        "\n",
        "# Pull a model (llama3 is recommended)\n",
        "!ollama pull llama3\n",
        "\n",
        "print(\"‚úÖ Ollama is ready! RAGScore will auto-detect it.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Quick Test Your RAG\n",
        "\n",
        "The `quick_test()` function generates QA pairs from your documents and evaluates your RAG in one call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragscore import quick_test\n",
        "\n",
        "# Test with an HTTP endpoint\n",
        "result = quick_test(\n",
        "    endpoint=\"http://localhost:8000/query\",  # Your RAG API\n",
        "    docs=\"docs/\",                            # Path to your documents\n",
        "    n=10,                                    # Number of test questions\n",
        "    threshold=0.7,                           # Pass if >= 70% correct\n",
        ")\n",
        "\n",
        "print(f\"Accuracy: {result.accuracy:.0%}\")\n",
        "print(f\"Passed: {result.passed}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test with a Function (No Server Needed!)\n",
        "\n",
        "You can pass a Python function directly - perfect for testing in notebooks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Test a simple RAG function\n",
        "def my_rag(question: str) -> str:\n",
        "    \"\"\"Your RAG implementation here.\"\"\"\n",
        "    # Replace with your actual RAG logic\n",
        "    # e.g., vectorstore.similarity_search(question)\n",
        "    return \"This is a placeholder answer.\"\n",
        "\n",
        "# Test it!\n",
        "result = quick_test(\n",
        "    endpoint=my_rag,  # Pass function directly\n",
        "    docs=\"docs/\",\n",
        "    n=5,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Analyze Results with Pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get results as DataFrame\n",
        "df = quick_test(\n",
        "    endpoint=\"http://localhost:8000/query\",\n",
        "    docs=\"docs/\",\n",
        "    n=10,\n",
        "    return_df=True,  # Return DataFrame instead of QuickTestResult\n",
        ")\n",
        "\n",
        "# View all results\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter to see only failures\n",
        "failures = df[df[\"is_correct\"] == False]\n",
        "print(f\"Found {len(failures)} failures:\")\n",
        "failures[[\"question\", \"score\", \"reason\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Export Corrections for RAG Improvement\n",
        "\n",
        "RAGScore identifies incorrect answers and provides corrections that can be injected back into your RAG system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragscore.quick_test import export_corrections\n",
        "\n",
        "# Run test and get corrections\n",
        "result = quick_test(\n",
        "    endpoint=\"http://localhost:8000/query\",\n",
        "    docs=\"docs/\",\n",
        "    n=20,\n",
        ")\n",
        "\n",
        "# View corrections\n",
        "print(f\"Found {len(result.corrections)} corrections:\")\n",
        "for c in result.corrections[:3]:\n",
        "    print(f\"\\nQ: {c['question'][:60]}...\")\n",
        "    print(f\"Wrong: {c['incorrect_answer'][:60]}...\")\n",
        "    print(f\"Correct: {c['correct_answer'][:60]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export corrections to file\n",
        "export_corrections(result, \"corrections.jsonl\")\n",
        "print(\"‚úÖ Corrections saved to corrections.jsonl\")\n",
        "print(\"Inject these into your RAG to improve accuracy!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Use in pytest (CI/CD)\n",
        "\n",
        "```python\n",
        "# test_rag.py\n",
        "from ragscore import quick_test\n",
        "\n",
        "def test_rag_accuracy():\n",
        "    result = quick_test(\n",
        "        endpoint=\"http://localhost:8000/query\",\n",
        "        docs=\"docs/\",\n",
        "        n=20,\n",
        "        threshold=0.8,\n",
        "        silent=True,\n",
        "    )\n",
        "    assert result.passed, f\"RAG accuracy too low: {result.accuracy:.0%}\"\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Full Pipeline: Generate + Evaluate\n",
        "\n",
        "For comprehensive evaluation, use the two-step pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragscore import run_pipeline, run_evaluation\n",
        "\n",
        "# Step 1: Generate QA pairs from documents\n",
        "run_pipeline(paths=[\"docs/\"], concurrency=5)\n",
        "# Output: output/generated_qas.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Evaluate RAG against generated QAs\n",
        "summary = run_evaluation(\n",
        "    golden_path=\"output/generated_qas.jsonl\",\n",
        "    endpoint=\"http://localhost:8000/query\",\n",
        "    output_path=\"results.json\",\n",
        ")\n",
        "\n",
        "print(f\"Accuracy: {summary.accuracy:.1%}\")\n",
        "print(f\"Average Score: {summary.avg_score:.1f}/5.0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìö Resources\n",
        "\n",
        "- **GitHub**: https://github.com/HZYAI/RagScore\n",
        "- **PyPI**: https://pypi.org/project/ragscore/\n",
        "- **Issues**: https://github.com/HZYAI/RagScore/issues\n",
        "\n",
        "‚≠ê Star us on GitHub if you find this useful!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
