{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ RAGScore Complete Demo: Build & Test a RAG in 5 Minutes\n",
    "\n",
    "This notebook shows the **complete workflow**:\n",
    "1. üìÑ Load a PDF (from URL or local file)\n",
    "2. üîß Build a minimal RAG with SentenceTransformers\n",
    "3. ‚úÖ Test it with RAGScore\n",
    "\n",
    "**No external servers needed** - everything runs in this notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q ragscore[notebook] sentence-transformers pypdf2\n",
    "\n",
    "# Safety net for Colab's event loop\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "print(\"‚úÖ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Your PDF\n",
    "\n",
    "Choose **Option A** (download from URL) or **Option B** (use local file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === OPTION A: Download a sample PDF from the web ===\n",
    "PDF_URL = \"https://arxiv.org/pdf/2005.11401.pdf\"  # RAG paper by Facebook\n",
    "\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "os.makedirs(\"docs\", exist_ok=True)\n",
    "pdf_path = \"docs/rag_paper.pdf\"\n",
    "\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(f\"üì• Downloading PDF from {PDF_URL}...\")\n",
    "    urllib.request.urlretrieve(PDF_URL, pdf_path)\n",
    "    print(f\"‚úÖ Saved to {pdf_path}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Using existing {pdf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === OPTION B: Use a local PDF (uncomment and modify) ===\n",
    "# pdf_path = \"/content/your_document.pdf\"  # Change this path\n",
    "# print(f\"Using local file: {pdf_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Text from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "from ragscore.data_processing import chunk_text\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Extract all text from a PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() or \"\"\n",
    "    return text\n",
    "\n",
    "# Extract and chunk\n",
    "raw_text = extract_text_from_pdf(pdf_path)\n",
    "chunks = chunk_text(raw_text)\n",
    "\n",
    "print(f\"üìÑ Extracted {len(raw_text):,} characters\")\n",
    "print(f\"üì¶ Created {len(chunks)} chunks\")\n",
    "print(f\"\\nüìù Sample chunk:\\n{chunks[0][:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build a Minimal RAG\n",
    "\n",
    "This is a simple but functional RAG using:\n",
    "- **SentenceTransformers** for embeddings\n",
    "- **Cosine similarity** for retrieval\n",
    "- **Top-k chunks** as context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "class SimpleRAG:\n",
    "    \"\"\"\n",
    "    A minimal RAG implementation for demonstration.\n",
    "\n",
    "    In production, you'd use:\n",
    "    - A vector database (Pinecone, Weaviate, Chroma)\n",
    "    - An LLM for answer generation (GPT-4, Claude, Llama)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, chunks: list[str], model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        print(f\"üîß Loading embedding model: {model_name}\")\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.chunks = chunks\n",
    "\n",
    "        print(f\"üìä Encoding {len(chunks)} chunks...\")\n",
    "        self.embeddings = self.model.encode(chunks, show_progress_bar=True)\n",
    "        print(\"‚úÖ RAG ready!\")\n",
    "\n",
    "    def retrieve(self, question: str, top_k: int = 3) -> list[str]:\n",
    "        \"\"\"Retrieve the most relevant chunks for a question.\"\"\"\n",
    "        q_embedding = self.model.encode([question])[0]\n",
    "\n",
    "        # Cosine similarity\n",
    "        scores = np.dot(self.embeddings, q_embedding) / (\n",
    "            np.linalg.norm(self.embeddings, axis=1) * np.linalg.norm(q_embedding)\n",
    "        )\n",
    "\n",
    "        # Get top-k indices\n",
    "        top_indices = np.argsort(scores)[-top_k:][::-1]\n",
    "        return [self.chunks[i] for i in top_indices]\n",
    "\n",
    "    def query(self, question: str) -> str:\n",
    "        \"\"\"\n",
    "        Answer a question using retrieved context.\n",
    "\n",
    "        Note: This simple version just returns the best chunk.\n",
    "        A real RAG would pass this to an LLM for synthesis.\n",
    "        \"\"\"\n",
    "        relevant_chunks = self.retrieve(question, top_k=1)\n",
    "\n",
    "        # Simple answer: return the most relevant chunk\n",
    "        # In production: send to LLM with prompt like:\n",
    "        # f\"Context: {relevant_chunks}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "        return relevant_chunks[0] if relevant_chunks else \"No relevant information found.\"\n",
    "\n",
    "# Build the RAG\n",
    "rag = SimpleRAG(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test the RAG Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a few questions\n",
    "test_questions = [\n",
    "    \"What is RAG?\",\n",
    "    \"How does retrieval work?\",\n",
    "    \"What are the benefits of RAG?\",\n",
    "]\n",
    "\n",
    "for q in test_questions:\n",
    "    print(f\"\\n‚ùì {q}\")\n",
    "    answer = rag.query(q)\n",
    "    print(f\"üí¨ {answer[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. üéØ Test with RAGScore!\n",
    "\n",
    "Now let's use RAGScore to systematically evaluate this RAG.\n",
    "\n",
    "### ‚ö†Ô∏è Expected: Low Scores (~1-2/5)\n",
    "\n",
    "**This is intentional!** Our `SimpleRAG` only retrieves text chunks - it doesn't use an LLM to synthesize answers. This is called **\"retrieval-only\"** and is just half of a real RAG system.\n",
    "\n",
    "| Component | SimpleRAG | Production RAG |\n",
    "|-----------|-----------|----------------|\n",
    "| Retrieval | ‚úÖ Yes | ‚úÖ Yes |\n",
    "| LLM Synthesis | ‚ùå No | ‚úÖ Yes (GPT-4, Claude, Llama) |\n",
    "| Expected Score | 1-2/5 | 4-5/5 |\n",
    "\n",
    "**The low scores demonstrate that RAGScore correctly identifies a weak RAG.** In production, you'd add LLM synthesis to generate coherent answers from the retrieved chunks, and scores would improve dramatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set your LLM API key for RAGScore's QA generation and judging\n",
    "# Option 1: OpenAI\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"  # Replace with your key\n",
    "\n",
    "# Option 2: Use local Ollama (uncomment below)\n",
    "# !curl -fsSL https://ollama.com/install.sh | sh\n",
    "# import subprocess, time\n",
    "# subprocess.Popen(\"ollama serve\", shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "# time.sleep(5)\n",
    "# !ollama pull llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragscore import quick_test\n",
    "\n",
    "# Run RAGScore evaluation!\n",
    "result = quick_test(\n",
    "    endpoint=rag.query,  # Pass the RAG function directly\n",
    "    docs=pdf_path,       # Use the same PDF\n",
    "    n=10,                # Generate 10 test questions\n",
    "    threshold=0.7,       # Pass if 70%+ correct\n",
    ")\n",
    "\n",
    "# Show the visualization\n",
    "result.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed metrics\n",
    "print(\"üìä Results Summary\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Accuracy: {result.accuracy:.1%}\")\n",
    "print(f\"Average Score: {result.avg_score:.1f}/5.0\")\n",
    "print(f\"Passed: {'‚úÖ Yes' if result.passed else '‚ùå No'}\")\n",
    "print(f\"Corrections needed: {len(result.corrections)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View all results as DataFrame\n",
    "result.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect failures\n",
    "bad_rows = result.df[result.df['score'] < 4]\n",
    "if len(bad_rows) > 0:\n",
    "    print(f\"‚ùå {len(bad_rows)} questions scored below 4:\\n\")\n",
    "    for _, row in bad_rows.iterrows():\n",
    "        print(f\"Q: {row['question'][:80]}...\")\n",
    "        print(f\"Score: {row['score']}/5 - {row['reason']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Corrections\n",
    "\n",
    "Save the corrections to improve your RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragscore.quick_test import export_corrections\n",
    "\n",
    "if result.corrections:\n",
    "    export_corrections(result, \"corrections.jsonl\")\n",
    "    print(\"‚úÖ Corrections saved to corrections.jsonl\")\n",
    "    print(\"\\nüìù Sample correction:\")\n",
    "    c = result.corrections[0]\n",
    "    print(f\"Q: {c['question'][:60]}...\")\n",
    "    print(f\"Wrong: {c['incorrect_answer'][:60]}...\")\n",
    "    print(f\"Correct: {c['correct_answer'][:60]}...\")\n",
    "else:\n",
    "    print(\"üéâ No corrections needed - your RAG is perfect!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì What's Next?\n",
    "\n",
    "This demo used a **minimal RAG** (just retrieval, no LLM synthesis). To improve:\n",
    "\n",
    "1. **Add an LLM** - Use GPT-4, Claude, or Llama to synthesize answers from retrieved chunks\n",
    "2. **Better chunking** - Use semantic chunking or sentence-level splitting\n",
    "3. **Vector database** - Use Chroma, Pinecone, or Weaviate for production\n",
    "4. **Reranking** - Add a cross-encoder reranker for better retrieval\n",
    "\n",
    "### Resources\n",
    "- **RAGScore GitHub**: https://github.com/HZYAI/RagScore\n",
    "- **RAGScore PyPI**: https://pypi.org/project/ragscore/\n",
    "\n",
    "‚≠ê Star us on GitHub if you found this useful!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
