"""
RAGScore Evaluation Module

Evaluates RAG system outputs against golden QA pairs using LLM-as-judge.
"""

import asyncio
import json
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Optional, Union

import aiohttp

from .exceptions import RAGScoreError
from .llm import detect_language, safe_json_parse
from .ui import get_async_pbar, patch_asyncio

_DETAILED_METRICS = [
    "correctness",
    "completeness",
    "relevance",
    "conciseness",
    "faithfulness",
]


@dataclass
class EvaluationResult:
    """Result of evaluating a single QA pair."""

    id: str
    question: str
    golden_answer: str
    rag_answer: str
    score: int  # 1-5
    reason: str
    is_correct: bool  # score >= 4
    correctness: Optional[int] = None
    completeness: Optional[int] = None
    relevance: Optional[int] = None
    conciseness: Optional[int] = None
    faithfulness: Optional[int] = None

    def to_dict(self) -> dict[str, Any]:
        d = {
            "id": self.id,
            "question": self.question,
            "golden_answer": self.golden_answer,
            "rag_answer": self.rag_answer,
            "score": self.score,
            "reason": self.reason,
            "is_correct": self.is_correct,
        }
        for metric in _DETAILED_METRICS:
            val = getattr(self, metric, None)
            if val is not None:
                d[metric] = val
        return d


@dataclass
class EvaluationSummary:
    """Summary of evaluation results."""

    total: int = 0
    correct: int = 0
    incorrect: int = 0
    accuracy: float = 0.0
    avg_score: float = 0.0
    results: list[EvaluationResult] = field(default_factory=list)

    def to_dict(self) -> dict[str, Any]:
        incorrect_pairs = [r.to_dict() for r in self.results if not r.is_correct]
        return {
            "summary": {
                "total": self.total,
                "correct": self.correct,
                "incorrect": self.incorrect,
                "accuracy": round(self.accuracy, 4),
                "avg_score": round(self.avg_score, 2),
            },
            "incorrect_pairs": incorrect_pairs,
        }


class RAGClient:
    """Client for calling RAG endpoints."""

    def __init__(
        self,
        endpoint: str,
        method: str = "POST",
        question_field: str = "question",
        answer_field: str = "answer",
        headers: Optional[dict[str, str]] = None,
        timeout: int = 30,
    ):
        """
        Initialize RAG client.

        Args:
            endpoint: RAG API endpoint URL
            method: HTTP method (POST or GET)
            question_field: Field name for question in request body
            answer_field: Field name for answer in response
            headers: Optional HTTP headers
            timeout: Request timeout in seconds
        """
        self.endpoint = endpoint
        self.method = method.upper()
        self.question_field = question_field
        self.answer_field = answer_field
        self.headers = headers or {"Content-Type": "application/json"}
        self.timeout = aiohttp.ClientTimeout(total=timeout)

    async def query(self, question: str, session: aiohttp.ClientSession) -> str:
        """
        Query the RAG endpoint with a question.

        Args:
            question: The question to ask
            session: aiohttp session for connection pooling

        Returns:
            The RAG system's answer
        """
        try:
            if self.method == "POST":
                payload = {self.question_field: question}
                async with session.post(
                    self.endpoint, json=payload, headers=self.headers, timeout=self.timeout
                ) as response:
                    response.raise_for_status()
                    data = await response.json()
            else:  # GET
                params = {self.question_field: question}
                async with session.get(
                    self.endpoint, params=params, headers=self.headers, timeout=self.timeout
                ) as response:
                    response.raise_for_status()
                    data = await response.json()

            # Extract answer from response
            answer = data.get(self.answer_field, "")
            if not answer and isinstance(data, dict):
                # Try common alternative field names
                for alt_field in ["response", "text", "content", "result"]:
                    if alt_field in data:
                        answer = data[alt_field]
                        break

            return str(answer) if answer else ""

        except aiohttp.ClientError as e:
            raise RAGScoreError(f"RAG endpoint error: {e}") from e
        except Exception as e:
            raise RAGScoreError(f"Failed to query RAG endpoint: {e}") from e


def _build_judge_prompt(
    question: str, golden_answer: str, rag_answer: str, lang: str, detailed: bool = False
) -> str:
    """Build the LLM-as-judge prompt."""
    if detailed:
        return _build_detailed_judge_prompt(question, golden_answer, rag_answer, lang)

    if lang == "zh":
        return f"""æ¯”è¾ƒRAGç³»ç»Ÿçš„å›ç­”ä¸æ ‡å‡†ç­”æ¡ˆã€‚

é—®é¢˜: {question}
æ ‡å‡†ç­”æ¡ˆ: {golden_answer}
RAGå›ç­”: {rag_answer}

è¯„åˆ†æ ‡å‡† (1-5åˆ†):
- 5: å®Œå…¨æ­£ç¡®ï¼Œè¯­ä¹‰ç­‰ä»·
- 4: åŸºæœ¬æ­£ç¡®ï¼Œæœ‰è½»å¾®é—æ¼
- 3: éƒ¨åˆ†æ­£ç¡®ï¼Œæœ‰ä¸€äº›é”™è¯¯
- 2: å¤§éƒ¨åˆ†é”™è¯¯ï¼Œæœ‰é‡å¤§é—®é¢˜
- 1: å®Œå…¨é”™è¯¯æˆ–æ— å…³

è¯·è¾“å‡ºJSONæ ¼å¼: {{"score": åˆ†æ•°, "reason": "ç®€çŸ­è§£é‡Š"}}"""
    else:
        return f"""Compare the RAG answer to the golden answer for this question.

Question: {question}
Golden Answer: {golden_answer}
RAG Answer: {rag_answer}

Score 1-5:
- 5: Fully correct, semantically equivalent
- 4: Mostly correct, minor omissions
- 3: Partially correct, some errors
- 2: Mostly incorrect, major errors
- 1: Completely wrong or irrelevant

Output JSON: {{"score": N, "reason": "brief explanation"}}"""


def _build_detailed_judge_prompt(
    question: str, golden_answer: str, rag_answer: str, lang: str
) -> str:
    """Build the detailed multi-metric LLM-as-judge prompt."""
    if lang == "zh":
        return f"""ä½ æ˜¯ä¸€ä¸ªå…¬æ­£çš„è¯„å®¡ï¼Œå¯¹RAGç³»ç»Ÿçš„å›ç­”è¿›è¡Œå¤šç»´åº¦è¯„ä¼°ã€‚

é—®é¢˜: {question}
æ ‡å‡†ç­”æ¡ˆ: {golden_answer}
RAGå›ç­”: {rag_answer}

è¯·ä»ä»¥ä¸‹5ä¸ªç»´åº¦è¯„åˆ† (æ¯é¡¹1-5åˆ†):

1. correctness (æ­£ç¡®æ€§): å›ç­”ä¸æ ‡å‡†ç­”æ¡ˆçš„è¯­ä¹‰ä¸€è‡´ç¨‹åº¦
   5=å®Œå…¨æ­£ç¡® 4=åŸºæœ¬æ­£ç¡® 3=éƒ¨åˆ†æ­£ç¡® 2=å¤§éƒ¨åˆ†é”™è¯¯ 1=å®Œå…¨é”™è¯¯

2. completeness (å®Œæ•´æ€§): å›ç­”æ˜¯å¦æ¶µç›–äº†æ ‡å‡†ç­”æ¡ˆä¸­çš„æ‰€æœ‰å…³é”®ä¿¡æ¯
   5=å®Œå…¨è¦†ç›– 4=è½»å¾®é—æ¼ 3=é—æ¼éƒ¨åˆ†è¦ç‚¹ 2=é—æ¼å¤§é‡ä¿¡æ¯ 1=å‡ ä¹æœªè¦†ç›–

3. relevance (ç›¸å…³æ€§): å›ç­”æ˜¯å¦é’ˆå¯¹æ‰€æé—®é¢˜
   5=å®Œå…¨åˆ‡é¢˜ 4=åŸºæœ¬åˆ‡é¢˜ 3=éƒ¨åˆ†åé¢˜ 2=å¤§éƒ¨åˆ†åé¢˜ 1=å®Œå…¨æ— å…³

4. conciseness (ç®€æ´æ€§): å›ç­”æ˜¯å¦ç®€æ´ï¼Œæ²¡æœ‰å¤šä½™æˆ–æ— å…³çš„ä¿¡æ¯
   5=ç®€æ´ç²¾å‡† 4=ç•¥æœ‰å†—ä½™ 3=æœ‰æ˜æ˜¾å†—ä½™ 2=å¤§é‡æ— å…³å†…å®¹ 1=å®Œå…¨å†—ä½™

5. faithfulness (å¿ å®åº¦): å›ç­”æ˜¯å¦å¿ å®äºæ ‡å‡†ç­”æ¡ˆä¸­çš„ä¿¡æ¯ï¼Œæ²¡æœ‰ç¼–é€ å†…å®¹
   5=å®Œå…¨å¿ å® 4=åŸºæœ¬å¿ å® 3=æœ‰ä¸€äº›ä¸ç¡®å®šå†…å®¹ 2=æœ‰æ˜æ˜¾ç¼–é€ ä¿¡æ¯ 1=å¤§é‡ç¼–é€ ä¿¡æ¯

è¯·è¾“å‡ºJSONæ ¼å¼:
{{"score": ç»¼åˆåˆ†æ•°, "reason": "ç®€çŸ­è§£é‡Š", "correctness": åˆ†æ•°, "completeness": åˆ†æ•°, "relevance": åˆ†æ•°, "conciseness": åˆ†æ•°, "faithfulness": åˆ†æ•°}}"""
    else:
        return f"""You are an impartial judge evaluating a RAG system answer across multiple dimensions.

Question: {question}
Golden Answer: {golden_answer}
RAG Answer: {rag_answer}

Score each dimension 1-5:

1. correctness: How semantically close is the answer to the golden answer?
   5=Fully correct  4=Mostly correct  3=Partially correct  2=Mostly wrong  1=Completely wrong

2. completeness: Does the answer cover all key points from the golden answer?
   5=Fully covered  4=Minor omissions  3=Some key points missing  2=Major gaps  1=Almost nothing covered

3. relevance: Does the answer actually address the question asked?
   5=Perfectly on-topic  4=Mostly on-topic  3=Partially off-topic  2=Mostly off-topic  1=Completely irrelevant

4. conciseness: Is the answer focused without unnecessary or irrelevant information?
   5=Concise and precise  4=Slightly verbose  3=Noticeably verbose  2=Mostly filler  1=Entirely off-track

5. faithfulness: Is the answer faithful to the golden answer without fabricating information?
   5=Fully faithful  4=Mostly faithful  3=Some unsupported claims  2=Significant fabrication  1=Mostly fabricated

Output JSON: {{"score": N, "reason": "brief explanation", "correctness": N, "completeness": N, "relevance": N, "conciseness": N, "faithfulness": N}}"""


async def _judge_single(
    qa: dict[str, Any],
    rag_answer: str,
    provider,
    semaphore: asyncio.Semaphore,
    detailed: bool = False,
) -> EvaluationResult:
    """Judge a single QA pair."""
    question = qa.get("question", "")
    golden_answer = qa.get("answer", "")
    qa_id = qa.get("id", "unknown")

    # Detect language from question
    lang = detect_language(question)

    # Build judge prompt
    user_prompt = _build_judge_prompt(question, golden_answer, rag_answer, lang, detailed=detailed)

    system_prompt = (
        "You are an impartial judge evaluating RAG system answers. "
        "Be strict but fair. Output only valid JSON."
    )

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt},
    ]

    async with semaphore:
        try:
            response = await provider.agenerate(messages=messages, temperature=0.3, json_mode=True)
            data = safe_json_parse(response.content)
            score = int(data.get("score", 1))
            reason = data.get("reason", "No reason provided")
        except Exception as e:
            # Default to low score on error
            score = 1
            reason = f"Evaluation error: {e}"
            data = {}

    # Clamp score to valid range
    score = max(1, min(5, score))

    # Extract detailed metrics
    metric_kwargs = {}
    if detailed:
        for metric in _DETAILED_METRICS:
            val = data.get(metric)
            if val is not None:
                metric_kwargs[metric] = max(1, min(5, int(val)))

    return EvaluationResult(
        id=qa_id,
        question=question,
        golden_answer=golden_answer,
        rag_answer=rag_answer,
        score=score,
        reason=reason,
        is_correct=(score >= 4),
        **metric_kwargs,
    )


async def evaluate_rag(
    golden_qas: list[dict[str, Any]],
    rag_client: RAGClient,
    provider=None,
    concurrency: int = 5,
    correct_threshold: int = 4,
    detailed: bool = False,
) -> EvaluationSummary:
    """
    Evaluate RAG system against golden QA pairs.

    Args:
        golden_qas: List of golden QA pairs (must have 'question' and 'answer')
        rag_client: RAGClient instance for querying the RAG endpoint
        provider: LLM provider for judging (auto-detected if None)
        concurrency: Max concurrent requests (default: 5)
        correct_threshold: Score threshold for "correct" (default: 4)
        detailed: Enable multi-metric evaluation (default: False)

    Returns:
        EvaluationSummary with results and incorrect pairs
    """
    if provider is None:
        from .providers import get_provider

        provider = get_provider()

    semaphore = asyncio.Semaphore(concurrency)

    # Query RAG + Judge in single pipeline (parallel)
    async with aiohttp.ClientSession() as session:

        async def process_qa(qa: dict[str, Any]) -> EvaluationResult:
            question = qa.get("question", "")

            # Query RAG endpoint
            async with semaphore:
                try:
                    rag_answer = await rag_client.query(question, session)
                except Exception as e:
                    rag_answer = f"[ERROR: {e}]"

            # Immediately judge the answer
            return await _judge_single(qa, rag_answer, provider, semaphore, detailed=detailed)

        tasks = [process_qa(qa) for qa in golden_qas]
        async_pbar = get_async_pbar()
        results: list[EvaluationResult] = await async_pbar.gather(*tasks, desc="Evaluating")

    # Calculate summary
    total = len(results)
    correct = sum(1 for r in results if r.is_correct)
    incorrect = total - correct
    avg_score = sum(r.score for r in results) / total if total > 0 else 0.0
    accuracy = correct / total if total > 0 else 0.0

    return EvaluationSummary(
        total=total,
        correct=correct,
        incorrect=incorrect,
        accuracy=accuracy,
        avg_score=avg_score,
        results=results,
    )


def load_golden_qas(path: Union[str, Path]) -> list[dict[str, Any]]:
    """
    Load golden QA pairs from a JSONL file.

    Args:
        path: Path to JSONL file

    Returns:
        List of QA pair dictionaries
    """
    path = Path(path)
    if not path.exists():
        raise FileNotFoundError(f"Golden QA file not found: {path}")

    qas = []
    with open(path, encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line:
                try:
                    qa = json.loads(line)
                    if "question" in qa and "answer" in qa:
                        qas.append(qa)
                except json.JSONDecodeError:
                    continue

    if not qas:
        raise ValueError(f"No valid QA pairs found in {path}")

    return qas


def run_evaluation(
    golden_path: Union[str, Path],
    endpoint: str,
    output_path: Optional[Union[str, Path]] = None,
    concurrency: int = 5,
    question_field: str = "question",
    answer_field: str = "answer",
    method: str = "POST",
    headers: Optional[dict[str, str]] = None,
    model: Optional[str] = None,
    provider: Optional[str] = None,
    detailed: bool = False,
) -> EvaluationSummary:
    """
    Run RAG evaluation pipeline (synchronous wrapper).

    Args:
        golden_path: Path to golden QA pairs JSONL file
        endpoint: RAG API endpoint URL
        output_path: Optional path to save results JSON
        concurrency: Max concurrent requests
        question_field: Field name for question in RAG request
        answer_field: Field name for answer in RAG response
        method: HTTP method (POST or GET)
        headers: Optional HTTP headers for RAG endpoint
        model: LLM model for judging (auto-detected if None)
        provider: LLM provider for judging (e.g., 'ollama', 'openai'). Auto-detected if None.
        detailed: Enable multi-metric evaluation (default: False)

    Returns:
        EvaluationSummary with results
    """
    # Load golden QAs
    print(f"Loading golden QA pairs from {golden_path}...")
    golden_qas = load_golden_qas(golden_path)
    print(f"Loaded {len(golden_qas)} QA pairs")

    # Create RAG client
    rag_client = RAGClient(
        endpoint=endpoint,
        method=method,
        question_field=question_field,
        answer_field=answer_field,
        headers=headers,
    )

    # Get LLM provider for judging
    from .providers import get_provider

    llm_provider = get_provider(provider=provider, model=model)

    # Patch asyncio for notebook environments
    patch_asyncio()

    # Run evaluation
    print(f"\nEvaluating against RAG endpoint: {endpoint}")
    print(f"Judge LLM: {llm_provider.provider_name} ({llm_provider.model})")
    print(f"Concurrency: {concurrency}")

    summary = asyncio.run(
        evaluate_rag(
            golden_qas=golden_qas,
            rag_client=rag_client,
            provider=llm_provider,
            concurrency=concurrency,
            detailed=detailed,
        )
    )

    # Print summary
    print(f"\n{'=' * 60}")
    if summary.accuracy >= 0.9:
        status = "âœ… EXCELLENT"
    elif summary.accuracy >= 0.7:
        status = "ğŸ‘ GOOD"
    elif summary.accuracy >= 0.5:
        status = "âš ï¸  NEEDS IMPROVEMENT"
    else:
        status = "âŒ POOR"

    print(f"{status}: {summary.correct}/{summary.total} correct ({summary.accuracy * 100:.1f}%)")
    print(f"Average Score: {summary.avg_score:.2f}/5.0")

    if detailed and summary.results:
        separator = "\u2500" * 60
        print(separator)
        for metric in _DETAILED_METRICS:
            vals = [
                getattr(r, metric) for r in summary.results if getattr(r, metric, None) is not None
            ]
            if vals:
                avg = sum(vals) / len(vals)
                label = metric.replace("_", " ").title()
                print(f"  {label}: {avg:.2f}/5.0")

    print(f"{'=' * 60}")

    # Print incorrect pairs
    incorrect = [r for r in summary.results if not r.is_correct]
    if incorrect:
        print(f"\nâŒ {len(incorrect)} Incorrect Pairs:\n")
        for i, r in enumerate(incorrect[:10], 1):  # Show first 10
            q_preview = r.question[:60] + "..." if len(r.question) > 60 else r.question
            print(f'  {i}. Q: "{q_preview}"')
            print(f"     Score: {r.score}/5 - {r.reason}")
            print()

        if len(incorrect) > 10:
            print(f"  ... and {len(incorrect) - 10} more (use --output to save all)")

    # Save results
    if output_path:
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(summary.to_dict(), f, ensure_ascii=False, indent=2)
        print(f"\nğŸ“„ Full results saved to {output_path}")
    elif incorrect:
        print("ğŸ’¡ Tip: Use --output results.json to save all incorrect pairs")

    print("\nâ­ Enjoying RAGScore? Star us: https://github.com/HZYAI/RagScore")
    print("ğŸ’¬ Questions? Join discussions: https://github.com/HZYAI/RagScore/discussions")

    return summary
